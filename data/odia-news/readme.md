
## openwebtext dataset

after running `prepare.py` (preprocess) we get:


- train.bin is ~377MB, val.bin ~8.5MB
- train has ~197M tokens (197,386,239)
- val has ~8M tokens (8,171,949)

this came from 243,231 documents in total.

references:

- OpenAI's WebText dataset is discussed in [GPT-2 paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [tripathysagar/odia-news](https://huggingface.co/datasets/tripathysagar/odia-news/viewer/default/test?p=1) dataset
